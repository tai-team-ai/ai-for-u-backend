from pathlib import Path
import sys
import logging
import json
from typing import Optional
from uuid import UUID
from fastapi import APIRouter, Request, status
from typing import Any
from pynamodb.models import Model



sys.path.append(Path(__file__, "../utils").absolute())
sys.path.append(Path(__file__, "../gpt_turbo").absolute())
sys.path.append(Path(__file__, "../dynamodb_models").absolute())
from utils import CamelCaseModel, UUID_HEADER_NAME
from gpt_turbo import GPTTurboChatSession, get_gpt_turbo_response, GPTTurboChat, Role
from dynamodb_models import AuthenticatedUserData

router = APIRouter()

logger = logging.getLogger()
logger.setLevel(logging.INFO)

SYSTEM_PROMPT = (
    "You are a friendly assist named Roo. You are to act as someone who is friendly and "
    "helpful. You are to help the user with whatever they need help with but also be conversational. "
    "You are to be a good listener and ask how you can help and be there for them. You work for a "
    "site that is called AI for U which seeks to empower EVERYONE to feel comfortable using AI. "
    "You should assume the user is not technical unless they ask you a technical question.\n"
    "Most IMPORTANTLY, you need to ask questions to understand the user as best as possible. "
    "This will allow you to better understand who the person is and what they need help with. "
    "You should also ask questions to make sure you understand what the user is saying. "
    "You MUST get to know them as a human being and understand their needs in order to be successful."
)


class SandBoxChatHistory(GPTTurboChatSession):
    """
    Define the chat history for user sandbox-chatgpt sessions.

    ### Attributes:
        message_uuid: A unique identifier for the message.\n\n
    """
    chat_uuid: UUID


class SandBoxChatGPTRequest(CamelCaseModel):
    """
    ## Define the request body for sandbox-chatgpt endpoint.

    When a conversation is started, the client should send a request with a conversation_id and an empty prompt.
    The response will contain the initial prompt from the model. The client should then send a request with the 
    conversation_id for each subsequent prompt. The prompt should only include the user's response to the model's 
    prompt.

    ### Example Conversation:
    * Request 1: {"conversation_id": "id", "prompt": ""}\n\n
    * Response 1: {"gpt_response": "Hi, how are you?"}\n\n
    * Request 2: {"conversation_id": "id", "prompt": "I'm good, how are you?"}\n\n
    * Response 2: {"gpt_response": "I'm good, thanks for asking."}\n\n

    ### Attributes:
        conversation_uuid: A unique identifier for the conversation (generated by the client)
        user_message: The prompt to send to the model. The prompt should only include the user's response to the model's prompt.
    """

    conversation_uuid: UUID
    user_message: Optional[str] = ""


class SandBoxChatGPTResponse(CamelCaseModel):
    gpt_response: str


class SandBoxChatGPTExamplesResponse(CamelCaseModel):
    """
    ## Define example starter prompts for sandbox-chatgpt endpoint.

    ### Example:
    * example_names: ["How to Cook Ramen"] \n\n
    * examples: ["I want to cook ramen. What ingredients do I need?"]

    ### Attributes:
        example_names: List of example names. \n\n
        examples: List of corresponding example prompts.
    """
    example_names: list[str]
    examples: list[str]


@router.get("/sandbox-chatgpt-examples", response_model=SandBoxChatGPTExamplesResponse, status_code=status.HTTP_200_OK)
async def sandbox_chatgpt_examples() -> SandBoxChatGPTExamplesResponse:
    """
    Get examples for sandbox-chatgpt.

    Returns:
        examples: Examples for sandbox-chatgpt.
    """
    return SandBoxChatGPTExamplesResponse(
        example_names=["How to Cook Ramen"],
        examples=["I want to cook ramen. What ingredients do I need?"]
    )


def load_sandbox_chat_history(user_id: UUID, conversation_uuid: UUID) -> GPTTurboChatSession:
    """
    Load the chat history for a sandbox-chatgpt session.

    Args:
        conversation_uuid: A unique identifier for the conversation (generated by the client)

    Returns:
        chat_history: Chat history for a sandbox-chatgpt session.
    """
    chat_session_dict = {"chat_uuid": conversation_uuid}
    try:
        chat_history = AuthenticatedUserData.get(user_id).sandbox_chat_history
        chat_session_dict = chat_history.get(str(conversation_uuid), chat_session_dict)
    except Model.DoesNotExist:
        pass
    return GPTTurboChatSession(**chat_session_dict)

def save_sandbox_chat_history(user_uuid: UUID, sandbox_chat_history: SandBoxChatHistory) -> None:
    """
    Save the chat history for a sandbox-chatgpt session.

    Args:
        chat_session: Chat history for a sandbox-chatgpt session.
    """
    pass
    

@router.post("/sandbox-chatgpt", response_model=SandBoxChatGPTResponse, status_code=status.HTTP_200_OK)
async def sandbox_chatgpt(sandbox_chatgpt_request: SandBoxChatGPTRequest, request: Request) -> SandBoxChatGPTResponse:
    """
    Get response from openAI Turbo GPT-3 model.

    Args:
        sandbox_chatgpt_request: Request containing conversation_id and prompt.

    Returns:
        gpt_response: Response from openAI Turbo GPT-3 model.
    """
    uuid = request.headers.get(UUID_HEADER_NAME)
    logger.info("uuid: %s", uuid)
    chat_session = load_sandbox_chat_history(
        user_id=uuid,
        conversation_uuid=sandbox_chatgpt_request.conversation_uuid
    )
    logger.info("chat_session before response: %s", chat_session)

    chat_session = get_gpt_turbo_response(
        system_prompt=SYSTEM_PROMPT,
        chat_session=chat_session,
        frequency_penalty=0.9,
        temperature=0.9,
        uuid=uuid
    )
    logger.info("chat_session after response: %s", chat_session)
    save_sandbox_chat_history(user_uuid=uuid, sandbox_chat_history=chat_session)

    latest_gpt_chat_model = chat_session.messages[-1]
    latest_message = latest_gpt_chat_model.content
    return SandBoxChatGPTResponse(gpt_response=latest_message)